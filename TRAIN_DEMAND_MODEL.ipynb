{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfSKXCbJUwL9",
        "outputId": "232515ed-145b-4006-86b5-994ef0b31ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting demand forecasting model training...\n",
            "Loaded 57 historical incident records.\n",
            "Temporal features (hour, day_of_week, month) extracted.\n",
            "Training on 45 samples, testing on 12 samples.\n",
            "Model training completed.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Mean Squared Error (MSE): 5.6969\n",
            "Root Mean Squared Error (RMSE): 2.3868\n",
            "\n",
            "Trained demand model saved to: ../routing/demand_model.pkl\n",
            "\n",
            "Training process finished successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "def train_demand_model():\n",
        "    \"\"\"\n",
        "    Trains a Gradient Boosting Regressor to predict incident hotspots based on\n",
        "    geospatial and temporal data.\n",
        "    \"\"\"\n",
        "    print(\"Starting demand forecasting model training...\")\n",
        "\n",
        "    # --- 1. Load and Prepare Data ---\n",
        "    try:\n",
        "        # Correct path according to project structure\n",
        "        data_path = \"/content/historical_incidents_500.csv\"\n",
        "        df = pd.read_csv(data_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The dataset was not found at {data_path}\")\n",
        "        print(\"Please ensure 'historical_incidents_500.csv' is in the 'ai_models/routing/data/' directory.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loaded {len(df)} historical incident records.\")\n",
        "\n",
        "    # --- 2. Feature Engineering ---\n",
        "    # Convert timestamp to datetime objects\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    # Extract temporal features\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek # Monday=0, Sunday=6\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "\n",
        "    # For simplicity in this example, we'll create a synthetic target variable.\n",
        "    # In a real-world scenario, this would be a more complex aggregation,\n",
        "    # e.g., counting incidents per hour in a given geographical grid cell.\n",
        "    # Here, we'll just create a pseudo-random \"incident_count\" for demonstration.\n",
        "    df['incident_count'] = 1 + (df['hour'] // 6) + (df['day_of_week'] % 3) + df.index % 5\n",
        "\n",
        "    print(\"Temporal features (hour, day_of_week, month) extracted.\")\n",
        "\n",
        "    # Define features (X) and target (y)\n",
        "    features = ['latitude', 'longitude', 'hour', 'day_of_week', 'month']\n",
        "    target = 'incident_count'\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    # --- 3. Model Training ---\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    print(f\"Training on {len(X_train)} samples, testing on {len(X_test)} samples.\")\n",
        "\n",
        "    # Initialize and train the model\n",
        "    # A Gradient Boosting Regressor is a good choice for this kind of tabular prediction task.\n",
        "    gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "    gbr.fit(X_train, y_train)\n",
        "    print(\"Model training completed.\")\n",
        "\n",
        "    # --- 4. Evaluation ---\n",
        "    y_pred = gbr.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(\"\\n--- Model Evaluation ---\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {mse**0.5:.4f}\")\n",
        "\n",
        "    # --- 5. Serialization ---\n",
        "    # Save the trained model\n",
        "    output_dir = os.path.join('..', 'routing')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model_path = os.path.join(output_dir, 'demand_model.pkl')\n",
        "    joblib.dump(gbr, model_path)\n",
        "\n",
        "    print(f\"\\nTrained demand model saved to: {model_path}\")\n",
        "    print(\"\\nTraining process finished successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This script is intended to be run from within the `ai_models/routing` directory\n",
        "    # For example: `python train_demand_model.py`\n",
        "    train_demand_model()"
      ]
    }
  ]
}